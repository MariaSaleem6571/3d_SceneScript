{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Encoder (Using Sinusoidal Positional Encoding)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([433426, 7])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "df = pd.read_csv('/home/mseleem/Desktop/3d_SceneScript/0/semidense_points.csv.gz')\n",
    "df = df.apply(pd.to_numeric, errors='coerce')\n",
    "df = df.fillna(0) \n",
    "numpy_array = df.values\n",
    "tensor = torch.tensor(numpy_array, dtype=torch.float32)\n",
    "\n",
    "print(tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n",
      "ViT encoded features shape: torch.Size([1212, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from point_cloud_processor import PointCloudTransformerLayer\n",
    "\n",
    "# Instantiate the model\n",
    "model = PointCloudTransformerLayer().cuda()\n",
    "\n",
    "# Define the filepath to the point cloud data\n",
    "filepath = \"/home/mseleem/Desktop/3d_model_pt/0/semidense_points.csv.gz\"\n",
    "\n",
    "# Call the forward method to process the point cloud and get encoded features\n",
    "vit_encoded_features = model(filepath)\n",
    "\n",
    "# Print the shape of the encoded features\n",
    "print(f\"ViT encoded features shape: {vit_encoded_features.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded point cloud with 433426 points.\n",
      "Encoded features (F) from point cloud: torch.Size([1211, 512])\n",
      "Encoded features (C) from point cloud: torch.Size([1211, 4])\n",
      "ViT encoded features shape: torch.Size([1212, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import open3d as o3d\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from torch import nn\n",
    "import torchsparse\n",
    "import torchsparse.nn as spnn\n",
    "from torchsparse import SparseTensor\n",
    "\n",
    "def read_points_file(filepath):\n",
    "    assert os.path.exists(filepath), f\"Could not find point cloud file: {filepath}\"\n",
    "    df = pd.read_csv(filepath, compression=\"gzip\")\n",
    "    point_cloud = df[[\"px_world\", \"py_world\", \"pz_world\"]]\n",
    "    dist_std = df[\"dist_std\"]\n",
    "    print(f\"Loaded point cloud with {len(point_cloud)} points.\")\n",
    "    return point_cloud.to_numpy(), dist_std.to_numpy()\n",
    "\n",
    "def generate_sinusoidal_positional_encoding(coordinates, d_model):\n",
    "    \"\"\"\n",
    "    Generates a sinusoidal positional encoding matrix.\n",
    "    \"\"\"\n",
    "    n_positions, n_dims = coordinates.shape\n",
    "    pe = torch.zeros(n_positions, d_model).cuda()\n",
    "    position = coordinates.float().cuda()\n",
    "    div_term = torch.exp(torch.arange(0, d_model // n_dims, 2).float() * -(np.log(10000.0) / (d_model // n_dims))).cuda()\n",
    "\n",
    "    for i in range(n_dims):\n",
    "        pe[:, 2 * i:d_model:2 * n_dims] = torch.sin(position[:, i].unsqueeze(1) * div_term)\n",
    "        pe[:, 2 * i + 1:d_model:2 * n_dims] = torch.cos(position[:, i].unsqueeze(1) * div_term)\n",
    "\n",
    "    return pe\n",
    "\n",
    "class TransformerEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, heads, mlp_dim, dropout=0.1):\n",
    "        super(TransformerEncoderBlock, self).__init__()\n",
    "        self.norm1 = nn.LayerNorm(dim)\n",
    "        self.attn = nn.MultiheadAttention(dim, heads, dropout=dropout)\n",
    "        self.norm2 = nn.LayerNorm(dim)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(dim, mlp_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(mlp_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        x = x + self.ff(self.norm2(x))\n",
    "        return x\n",
    "\n",
    "class ViTEncoder(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.1):\n",
    "        super(ViTEncoder, self).__init__()\n",
    "        self.layers = nn.ModuleList([\n",
    "            TransformerEncoderBlock(dim, heads, mlp_dim, dropout) for _ in range(depth)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, dim, depth, heads, mlp_dim, dropout=0.1, emb_dropout=0.1):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        self.context_embedding = nn.Parameter(torch.randn(1, 1, dim))\n",
    "        self.dropout = nn.Dropout(emb_dropout)\n",
    "        self.encoder = ViTEncoder(dim, depth, heads, mlp_dim, dropout)\n",
    "        # self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, pos_encoding):\n",
    "        batch_size = x.shape[0]\n",
    "        cls_tokens = self.context_embedding.expand(batch_size, -1, -1)\n",
    "        x = torch.cat((cls_tokens, x), dim=1)\n",
    "        pos_encoding = torch.cat((torch.zeros(batch_size, 1, pos_encoding.size(-1)).cuda(), pos_encoding), dim=1)\n",
    "        x = x + pos_encoding\n",
    "        x = self.dropout(x)\n",
    "        x = self.encoder(x)\n",
    "        # x = self.norm(x)\n",
    "        return x\n",
    "\n",
    "class SparseResNetEncoder(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SparseResNetEncoder, self).__init__()\n",
    "        self.conv1 = spnn.Conv3d(1, 16, kernel_size=3, stride=2)\n",
    "        self.conv2 = spnn.Conv3d(16, 32, kernel_size=3, stride=2)\n",
    "        self.conv3 = spnn.Conv3d(32, 64, kernel_size=3, stride=2)\n",
    "        self.conv4 = spnn.Conv3d(64, 128, kernel_size=3, stride=2)\n",
    "        self.conv5 = spnn.Conv3d(128, 512, kernel_size=3, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        return x\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    voxel_size = 0.03 \n",
    "    points, dist_std = read_points_file(\"/home/mseleem/Desktop/3d_model_pt/0/semidense_points.csv.gz\")\n",
    "    pcd = o3d.geometry.PointCloud()\n",
    "    pcd.points = o3d.utility.Vector3dVector(points)\n",
    "    voxel_grid = o3d.geometry.VoxelGrid.create_from_point_cloud(pcd, voxel_size)\n",
    "\n",
    "    voxel_map = {}\n",
    "\n",
    "    for i, point in enumerate(points):\n",
    "        voxel_index = tuple(voxel_grid.get_voxel(point))\n",
    "        if voxel_index in voxel_map:\n",
    "            voxel_map[voxel_index].append(i)\n",
    "        else:\n",
    "            voxel_map[voxel_index] = [i]\n",
    "\n",
    "    aggregated_features = []\n",
    "    filtered_voxel_indices = []\n",
    "    for idx, (voxel_index, point_indices) in enumerate(voxel_map.items()):\n",
    "        aggregated_feature = np.mean(dist_std[point_indices])\n",
    "        aggregated_features.append(aggregated_feature)\n",
    "        filtered_voxel_indices.append(voxel_index)\n",
    "\n",
    "    voxel_indices_tensor = torch.tensor(filtered_voxel_indices, dtype=torch.int32).cuda()\n",
    "    features_tensor = torch.tensor(aggregated_features, dtype=torch.float32).view(-1, 1).cuda()\n",
    "\n",
    "    batch_indices = torch.zeros((voxel_indices_tensor.shape[0], 1), dtype=torch.int32).cuda()\n",
    "    voxel_indices_tensor_with_batch = torch.cat([batch_indices, voxel_indices_tensor], dim=1)\n",
    "\n",
    "    sparse_tensor = SparseTensor(features_tensor, voxel_indices_tensor_with_batch)\n",
    "\n",
    "    encoder = SparseResNetEncoder().cuda()\n",
    "    encoded_features = encoder(sparse_tensor.cuda())\n",
    "\n",
    "    print(f\"Encoded features (F) from point cloud: {encoded_features.F.shape}\")\n",
    "    print(f\"Encoded features (C) from point cloud: {encoded_features.C.shape}\")\n",
    "\n",
    "    positional_encoding = generate_sinusoidal_positional_encoding(encoded_features.C, 512)\n",
    "    encoded_features_with_pos = encoded_features.F + positional_encoding\n",
    "\n",
    "    vit = VisionTransformer(dim=512, depth=6, heads=8, mlp_dim=2048).cuda()\n",
    "    encoded_features_with_pos = encoded_features_with_pos.unsqueeze(0)\n",
    "    vit_encoded_features = vit(encoded_features_with_pos, positional_encoding.unsqueeze(0))\n",
    "\n",
    "    vit_encoded_features = vit_encoded_features.squeeze(0)\n",
    "    print(f\"ViT encoded features shape: {vit_encoded_features.shape}\")\n",
    "\n",
    "    # preprocessed_context_embedding = vit_encoded_features[0]  \n",
    "    # print(f\"Preprocessed context embedding shape: {preprocessed_context_embedding.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GT Script**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to parse the script and extract values\n",
    "def parse_script(file_path):\n",
    "    with open(file_path, 'r') as file:\n",
    "        lines = file.readlines()\n",
    "\n",
    "    data = []\n",
    "    for line in lines:\n",
    "        line = line.strip()\n",
    "        if line.startswith(\"make_wall\"):\n",
    "            command = \"make_wall\"\n",
    "            pattern = r'a_x=(-?\\d*\\.\\d+), a_y=(-?\\d*\\.\\d+), a_z=(-?\\d*\\.\\d+), b_x=(-?\\d*\\.\\d+), b_y=(-?\\d*\\.\\d+), b_z=(-?\\d*\\.\\d+), height=(-?\\d*\\.\\d+)'\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                values = [command] + list(map(float, match.groups()))\n",
    "                data.append(values)\n",
    "        \n",
    "        elif line.startswith(\"make_door\"):\n",
    "            command = \"make_door\"\n",
    "            pattern = r'wall0_id=(-?\\d+), wall1_id=(-?\\d+), position_x=(-?\\d*\\.\\d+), position_y=(-?\\d*\\.\\d+), position_z=(-?\\d*\\.\\d+), width=(-?\\d*\\.\\d+), height=(-?\\d*\\.\\d+)'\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                values = [command] + list(map(float, match.groups()))\n",
    "                data.append(values)\n",
    "\n",
    "        elif line.startswith(\"make_window\"):\n",
    "            command = \"make_window\"\n",
    "            pattern = r'wall0_id=(-?\\d+), wall1_id=(-?\\d+), position_x=(-?\\d*\\.\\d+), position_y=(-?\\d*\\.\\d+), position_z=(-?\\d*\\.\\d+), width=(-?\\d*\\.\\d+), height=(-?\\d*\\.\\d+)'\n",
    "            match = re.search(pattern, line)\n",
    "            if match:\n",
    "                values = [command] + list(map(float, match.groups()))\n",
    "                data.append(values)\n",
    "    \n",
    "    return data\n",
    "\n",
    "# File path\n",
    "file_path = '/home/mseleem/Desktop/3d_SceneScript/0/ase_scene_language.txt'\n",
    "\n",
    "# Parse the script\n",
    "data = parse_script(file_path)\n",
    "\n",
    "# Find the maximum number of columns required\n",
    "max_columns = max(len(row) for row in data)\n",
    "\n",
    "# Ensure all rows have the same number of columns by padding with None\n",
    "for row in data:\n",
    "    while len(row) < max_columns:\n",
    "        row.append(None)\n",
    "\n",
    "# Create a unified DataFrame\n",
    "columns = [\"command\"] + [\"parameter_\" + str(i) for i in range(1, max_columns)]\n",
    "df_all = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Convert DataFrame rows to vector embeddings\n",
    "vector_embeddings = df_all.values.tolist()\n",
    "\n",
    "# Display the vector embeddings\n",
    "print(\"Script Embeddings:\")\n",
    "for vector in vector_embeddings:\n",
    "    print(vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class Commands(Enum):\n",
    "    START = 1\n",
    "    STOP = 2\n",
    "    MAKE_WALL = 3\n",
    "    MAKE_WINDOW = 4\n",
    "    MAKE_DOOR = 5\n",
    "\n",
    "    @classmethod\n",
    "    def get_name_for(cls, value: int):\n",
    "        if value == cls.START.value:\n",
    "            return cls.START\n",
    "        if value == cls.STOP.value:\n",
    "            return cls.STOP\n",
    "        if value == cls.MAKE_WALL.value:\n",
    "            return cls.MAKE_WALL\n",
    "        if value == cls.MAKE_WINDOW.value:\n",
    "            return cls.MAKE_WINDOW\n",
    "        if value == cls.MAKE_DOOR.value:\n",
    "            return cls.MAKE_DOOR\n",
    "\n",
    "class TransformerOutputLayer(nn.Module):\n",
    "    def __init__(self, transformer_dim):\n",
    "        super(TransformerOutputLayer, self).__init__()\n",
    "        self.command_layer = nn.Linear(*transformer_dim, 5)  # Output 5 command logits (START included)\n",
    "        self.object_id_and_height_layer = nn.Linear(*transformer_dim, 2)  # Output the id and the height of the object\n",
    "        self.door_or_window_parameters_layer = nn.Linear(*transformer_dim, 6)  # Output the wall_id_0, wall_id_1, x, y, z, w of the door/window\n",
    "        self.wall_corners_layer = nn.Linear(*transformer_dim, 4)  # Output the x1, y1, x2, y2 of the wall\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the output from the transformer, shape: (batch_size, sequence_length, transformer_dim)\n",
    "        \n",
    "        # Predict commands\n",
    "        command_logits = self.command_layer(x)  # Shape: (batch_size, sequence_length, 3)\n",
    "        command_probs = F.softmax(command_logits, dim=-1)  # Shape: (batch_size, sequence_length, 3)\n",
    "        \n",
    "        object_id_and_height = self.object_id_and_height_layer(x)  # Shape: (batch_size, sequence_length, 2)\n",
    "\n",
    "        door_or_window_parameters = self.door_or_window_parameters_layer(x)  # Shape: (batch_size, sequence_length, 6)\n",
    "\n",
    "        wall_corners = self.wall_corners_layer(x)  # Shape: (batch_size, sequence_length, 4)\n",
    "        \n",
    "        return command_probs, object_id_and_height, door_or_window_parameters, wall_corners\n",
    "\n",
    "def select_parameters(command_probs, object_id_and_height, door_or_window_parameters, wall_corners):\n",
    "    # command_probs: shape (batch_size, sequence_length, 3)\n",
    "    # shared_parameter: shape (batch_size, sequence_length, 1)\n",
    "    # Get the predicted command indices (shape: batch_size, sequence_length)\n",
    "    command_indx = command_probs.argmax(dim=-1) + 2\n",
    "    command_indx = command_indx.numpy()\n",
    "    if command_indx == Commands.STOP.value:\n",
    "        parameters = torch.zeros(12)\n",
    "    elif command_indx == Commands.MAKE_WALL.value:\n",
    "        parameters = torch.cat((object_id_and_height, wall_corners))\n",
    "    elif command_indx in [Commands.MAKE_DOOR.value, Commands.MAKE_WINDOW.value]:\n",
    "        parameters = torch.cat((object_id_and_height, door_or_window_parameters))\n",
    "    \n",
    "    return Commands.get_name_for(command_indx), parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "command, parameters = select_parameters(torch.tensor([0, 1, 0, 0 ]), torch.zeros(2), torch.zeros(6), torch.zeros(4)+1)\n",
    "if parameters.size(0) < 8:\n",
    "    parameters = torch.cat((parameters, torch.zeros(8 - parameters.size(0))))\n",
    "print(command, parameters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "PARAM_SIZE = 8\n",
    "vocab = [\"START\", \"STOP\", \"make_wall\", \"make_window\", \"make_door\"]\n",
    "COMMANDS = vocab\n",
    "\n",
    "\n",
    "vocab_index = [1, 0, 2, 3, 4]\n",
    "# for command in [\"make_wall\", \"make_window\", \"make_door\"]:\n",
    "#     # for i in range(PARAM_SIZE):\n",
    "#     vocab.append(command)\n",
    "\n",
    "token_to_index = {token: idx for idx, token in enumerate(vocab)}\n",
    "index_to_token = {idx: token for token, idx in token_to_index.items()}\n",
    "\n",
    "\n",
    "VOCAB_SIZE = len(vocab) + PARAM_SIZE\n",
    "print(f\"Size of the vocabulary: {vocab_size}\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        # If no encoder is used, we need to provide the memory\n",
    "        # Normally memory would be the output from the encoder\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "# # Example usage\n",
    "# d_model = 512\n",
    "# nhead = 8\n",
    "# num_decoder_layers = 6\n",
    "# dim_feedforward = 2048\n",
    "# dropout = 0.1\n",
    "\n",
    "# decoder = CustomTransformerDecoder(d_model, nhead, num_decoder_layers, dim_feedforward, dropout)\n",
    "\n",
    "# # Example input tensors\n",
    "# tgt = torch.rand((10, 32, d_model))  # (sequence_length, batch_size, d_model)\n",
    "# memory = torch.rand((10, 32, d_model))  # Memory should come from somewhere (e.g., encoder output)\n",
    "\n",
    "# output = decoder(tgt, memory)\n",
    "# print(output.shape)\n",
    "\n",
    "\n",
    "def construct_embedding_vector_from_vocab(command: Commands, parameters: torch.Tensor):\n",
    "    num_classes = len(Commands)\n",
    "\n",
    "    # Convert the integer value to a tensor\n",
    "    value_tensor = torch.tensor(command.value - 1)\n",
    "\n",
    "    # Create the one-hot tensor\n",
    "    one_hot_tensor = F.one_hot(value_tensor, num_classes=num_classes)\n",
    "    if parameters.size(0) < 8:\n",
    "        parameters = torch.cat((parameters, torch.zeros(8 - parameters.size(0))))\n",
    "\n",
    "    return torch.cat((one_hot_tensor, parameters))\n",
    "\n",
    "\n",
    "class CommandTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512, nhead=8, num_layers=6):\n",
    "        super(CommandTransformer, self).__init__()\n",
    "        # self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.point_cloud_encoder = None #TODO: Add the encoder\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        self.transformer = CustomTransformerDecoder(d_model, nhead, num_layers, 2048)\n",
    "        self.output_layer = TransformerOutputLayer((12, d_model))\n",
    "\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        src_emb = self.point_cloud_encoder(src)\n",
    "        # tgt_emb = self.construct_embedding_vector_from_vocab(command, parameters)  # (seq_len, batch_size, d_model)\n",
    "        tgt_emb = self.pos_encoder(tgt_emb)\n",
    "        transformer_output = self.transformer(src_emb, tgt_emb)  # (tgt_seq_len, batch_size, d_model)\n",
    "        \n",
    "        # command_output = self.fc_command(transformer_output)  # (tgt_seq_len, batch_size, 3)\n",
    "        ouputs = self.output_layer(transformer_output)  # (tgt_seq_len, batch_size, vocab_size)\n",
    "        \n",
    "        return ouputs\n",
    "\n",
    "\n",
    "model = CommandTransformer(vocab_size=vocab_size)\n",
    "input_emb = construct_embedding_vector_from_vocab(Commands.START, torch.zeros(12)).unsqueeze(-1)\n",
    "point_cloud_tensor = torch.zeros((433426,6))\n",
    "\n",
    "while True:\n",
    "    pred = model(point_cloud_tensor, input_emb)\n",
    "    command, parameters = select_parameters(*pred)\n",
    "    output_emb = construct_embedding_vector_from_vocab(command, parameters)\n",
    "    input_emb = torch.cat(input_emb, output_emb.unsqueeze(-1))\n",
    "    if command == Commands.STOP:\n",
    "        break\n",
    "\n",
    "print(input_emb)\n",
    "\n",
    "# def generate_sequence(model):\n",
    "#     input_seq = torch.tensor([token_to_index[\"START\"]]).unsqueeze(1)  # Start with the START token\n",
    "#     print(input_seq.shape)\n",
    "#     tgt_seq = torch.tensor([token_to_index[\"START\"]]).unsqueeze(1)\n",
    "#     print(\"START\")\n",
    "#     sequence = []\n",
    "\n",
    "#     while True:\n",
    "#         with torch.no_grad():\n",
    "#             command_output, params_output = model(input_seq, tgt_seq)\n",
    "\n",
    "#         # Apply softmax to command_output to get probabilities\n",
    "#         command_probs = F.softmax(command_output[-1, 0], dim=-1)\n",
    "#         command_index = torch.argmax(command_probs).item()\n",
    "#         command_token = [\"make_wall\", \"make_window\", \"make_door\", \"STOP\"][command_index]\n",
    "\n",
    "#         if command_token == \"STOP\":\n",
    "#             print(\"STOP\")\n",
    "#             sequence.append(command_token)\n",
    "#             break\n",
    "\n",
    "#         # Print the predicted command and parameters\n",
    "#         print(command_token)\n",
    "\n",
    "#         params = params_output[-1, 0].tolist()\n",
    "#         print(params)\n",
    "\n",
    "#         sequence.append((command_token, params))\n",
    "\n",
    "#         # Update input sequence with the new command\n",
    "#         new_input = torch.tensor([token_to_index[command_token]]).unsqueeze(1)\n",
    "#         tgt_seq = torch.cat((tgt_seq, new_input), dim=0)\n",
    "#         input_seq = torch.cat((input_seq, new_input), dim=0)\n",
    "\n",
    "#     return sequence\n",
    "\n",
    "# def parse_predictions(predictions):\n",
    "#     sequence = [\"START\"]\n",
    "#     for command_token, params in predictions:\n",
    "#         sequence.append(command_token)\n",
    "#         if command_token != \"STOP\":\n",
    "#             sequence.extend(params)\n",
    "#     return sequence\n",
    "\n",
    "# # Generate and parse a sequence using the model\n",
    "# predictions = generate_sequence(model)\n",
    "# parsed_sequence = parse_predictions(predictions)\n",
    "\n",
    "# print(\"Generated Sequence:\")\n",
    "# print(parsed_sequence)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
