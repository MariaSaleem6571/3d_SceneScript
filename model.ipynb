{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenescriptprocessor import SceneScriptProcessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read point cloud: (433426, 3) (433426,)\n",
      "Encoded features: torch.Size([1211, 512]) Encoded coordinates: torch.Size([1211, 4])\n",
      "Positional encoding: torch.Size([1211, 512])\n",
      "Encoded features with positional encoding: torch.Size([1211, 512])\n",
      "Final encoded features: torch.Size([1, 1212, 512])\n"
     ]
    }
   ],
   "source": [
    "# model = PointCloudTransformerLayer().cuda()\n",
    "# pt_cloud_path = \"/home/mseleem/Desktop/3d_model_pt/0/semidense_points.csv.gz\"\n",
    "# pt_cloud_encoded_features = model(pt_cloud_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GT Script Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Walls DataFrame:\n",
      "     type   xcenter   ycenter  theta    width  height\n",
      "make_wall  1.251978  6.164665    0.0 7.634396 3.26243\n",
      "make_wall  5.069176  4.038439  -90.0 4.252452 3.26243\n",
      "make_wall  1.251978  1.912213  180.0 7.634396 3.26243\n",
      "make_wall -2.565221  4.038439   90.0 4.252452 3.26243\n",
      "make_wall -0.100031  1.766310    0.0 4.942956 3.26243\n",
      "make_wall  2.371447 -0.764045  -90.0 5.060709 3.26243\n",
      "make_wall -0.100031 -3.294399  180.0 4.942956 3.26243\n",
      "make_wall -2.571509 -0.764045   90.0 5.060709 3.26243\n",
      "\n",
      "Doors DataFrame:\n",
      "     type  position_x  position_y  position_z    width   height\n",
      "make_door   -1.511862    1.839261    1.011814 1.820626 2.023629\n",
      "make_door    2.870784    6.164665    0.993711 1.690708 1.987422\n",
      "make_door    5.069176    3.399479    0.983353 1.788263 1.966706\n",
      "\n",
      "Windows DataFrame:\n",
      "       type  position_x  position_y  position_z    width   height\n",
      "make_window    4.447797    6.164665    1.644804 1.007971 2.118911\n",
      "make_window   -2.565221    3.221880    1.375980 2.342133 2.336972\n",
      "make_window   -1.626487   -3.294399    2.249849 1.034733 1.210511\n",
      "make_window   -2.571509   -0.303761    1.696074 3.890121 2.829068\n",
      "Walls Vectors:\n",
      "[['make_wall' 1.2519775498658419 6.1646646447479725 0.0 7.634396325796843\n",
      "  3.262429714202881]\n",
      " ['make_wall' 5.069175712764263 4.0384385865181684 -90.0\n",
      "  4.252452116459608 3.262429714202881]\n",
      " ['make_wall' 1.2519775498658419 1.9122125282883644 180.0\n",
      "  7.634396325796843 3.262429714202881]\n",
      " ['make_wall' -2.5652206130325794 4.0384385865181684 90.0\n",
      "  4.252452116459608 3.262429714202881]\n",
      " ['make_wall' -0.10003073886036873 1.7663098871707916 0.0\n",
      "  4.9429556131362915 3.262429714202881]\n",
      " ['make_wall' 2.371447067707777 -0.7640445753931999 -90.0\n",
      "  5.060708925127983 3.262429714202881]\n",
      " ['make_wall' -0.10003073886036873 -3.2943990379571915 180.0\n",
      "  4.9429556131362915 3.262429714202881]\n",
      " ['make_wall' -2.5715085454285145 -0.7640445753931999 90.0\n",
      "  5.060708925127983 3.262429714202881]]\n",
      "\n",
      "Doors Vectors:\n",
      "[['make_door' -1.51186203956604 1.839261207729578 1.0118143235640777\n",
      "  1.820625901222229 2.0236286471281555]\n",
      " ['make_door' 2.870784282684326 6.1646646447479725 0.9937112153126255\n",
      "  1.6907079219818115 1.987422430625251]\n",
      " ['make_door' 5.069175712764263 3.399479389190674 0.9833531347230424\n",
      "  1.7882633209228516 1.9667062694460848]]\n",
      "\n",
      "Windows Vectors:\n",
      "[['make_window' 4.447796821594238 6.1646646447479725 1.6448044790145468\n",
      "  1.0079708099365234 2.118910707038747]\n",
      " ['make_window' -2.5652206130325794 3.221879720687866 1.3759804045958606\n",
      "  2.342133045196533 2.3369722195232856]\n",
      " ['make_window' -1.6264874935150146 -3.2943990379571915 2.249848967967298\n",
      "  1.0347331762313843 1.210511402716884]\n",
      " ['make_window' -2.5715085454285145 -0.30376124382019043\n",
      "  1.696074043500126 3.8901214599609375 2.829067691237224]]\n"
     ]
    }
   ],
   "source": [
    "gt_script= '/home/mseleem/Desktop/3d_SceneScript/0/ase_scene_language.txt' \n",
    "processor = SceneScriptProcessor(gt_script)\n",
    "processor.process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "from point_cloud_processor import PointCloudTransformerLayer, read_points_file\n",
    "import torch\n",
    "\n",
    "# Example usage for reading from a file:\n",
    "pt_cloud_path = \"/home/mseleem/Desktop/3d_model_pt/0/semidense_points.csv.gz\"\n",
    "point_cloud_df = read_points_file(pt_cloud_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Decoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/mseleem/Desktop/3d_SceneScript/model.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=123'>124</a>\u001b[0m input_emb \u001b[39m=\u001b[39m construct_embedding_vector_from_vocab(Commands\u001b[39m.\u001b[39mSTART, torch\u001b[39m.\u001b[39mzeros(\u001b[39m6\u001b[39m)\u001b[39m.\u001b[39mcuda())\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=125'>126</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=126'>127</a>\u001b[0m     pred \u001b[39m=\u001b[39m model(point_cloud_df, input_emb)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=127'>128</a>\u001b[0m     command, parameters \u001b[39m=\u001b[39m select_parameters(\u001b[39m*\u001b[39mpred)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=128'>129</a>\u001b[0m     output_emb \u001b[39m=\u001b[39m construct_embedding_vector_from_vocab(command, parameters)\u001b[39m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[1;32m/home/mseleem/Desktop/3d_SceneScript/model.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=113'>114</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, src: torch\u001b[39m.\u001b[39mTensor, tgt: torch\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=114'>115</a>\u001b[0m     src_emb \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpoint_cloud_encoder(src)\u001b[39m.\u001b[39mcuda()\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=115'>116</a>\u001b[0m     \u001b[39mprint\u001b[39m(src_emb\u001b[39m.\u001b[39mshape)\n\u001b[1;32m    <a href='vscode-notebook-cell://ssh-remote%2B192.168.102.17/home/mseleem/Desktop/3d_SceneScript/model.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=116'>117</a>\u001b[0m     \u001b[39m# tgt_emb = self.embedding_layer(tgt).cuda()\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/3d_SceneScript/point_cloud_processor.py:79\u001b[0m, in \u001b[0;36mPointCloudTransformerLayer.forward\u001b[0;34m(self, point_cloud_df)\u001b[0m\n\u001b[1;32m     75\u001b[0m dist_std \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor(dist_std, dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mfloat32, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     77\u001b[0m \u001b[39m# print(\"Received point cloud:\", points.shape, dist_std.shape)  \u001b[39;00m\n\u001b[0;32m---> 79\u001b[0m sparse_tensor \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mprocess_point_cloud(points, dist_std)\n\u001b[1;32m     81\u001b[0m encoded_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparse_encoder(sparse_tensor)\n\u001b[1;32m     82\u001b[0m \u001b[39m# print(\"Encoded features:\", encoded_features.F.shape, \"Encoded coordinates:\", encoded_features.C.shape) \u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/3d_SceneScript/point_cloud_processor.py:56\u001b[0m, in \u001b[0;36mPointCloudTransformerLayer.process_point_cloud\u001b[0;34m(self, points, dist_std)\u001b[0m\n\u001b[1;32m     54\u001b[0m filtered_voxel_indices \u001b[39m=\u001b[39m []\n\u001b[1;32m     55\u001b[0m \u001b[39mfor\u001b[39;00m idx, (voxel_index, point_indices) \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(voxel_map\u001b[39m.\u001b[39mitems()):\n\u001b[0;32m---> 56\u001b[0m     aggregated_feature \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mmean(dist_std[point_indices]\u001b[39m.\u001b[39;49mcpu()\u001b[39m.\u001b[39mnumpy())\n\u001b[1;32m     57\u001b[0m     aggregated_features\u001b[39m.\u001b[39mappend(aggregated_feature)\n\u001b[1;32m     58\u001b[0m     filtered_voxel_indices\u001b[39m.\u001b[39mappend(voxel_index)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from enum import Enum\n",
    "\n",
    "class Commands(Enum):\n",
    "    START = 1\n",
    "    STOP = 2\n",
    "    MAKE_WALL = 3\n",
    "    MAKE_WINDOW = 4\n",
    "    MAKE_DOOR = 5\n",
    "\n",
    "    @classmethod\n",
    "    def get_name_for(cls, value: int):\n",
    "        if value == cls.START.value:\n",
    "            return cls.START\n",
    "        if value == cls.STOP.value:\n",
    "            return cls.STOP\n",
    "        if value == cls.MAKE_WALL.value:\n",
    "            return cls.MAKE_WALL\n",
    "        if value == cls.MAKE_WINDOW.value:\n",
    "            return cls.MAKE_WINDOW\n",
    "        if value == cls.MAKE_DOOR.value:\n",
    "            return cls.MAKE_DOOR\n",
    "\n",
    "class TransformerOutputLayer(nn.Module):\n",
    "    def __init__(self, transformer_dim):\n",
    "        super(TransformerOutputLayer, self).__init__()\n",
    "        self.command_layer = nn.Linear(transformer_dim, 5)  # Output 5 command logits (START included)\n",
    "        self.param_layer = nn.Linear(transformer_dim, 6) # Output 6 params for wall, doors, windows \n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is the output from the transformer, shape: (batch_size, sequence_length, transformer_dim)\n",
    "        \n",
    "        # Predict commands\n",
    "        command_logits = self.command_layer(x)  # Shape: (batch_size, sequence_length, 5)\n",
    "        command_probs = F.softmax(command_logits, dim=-1)  # Shape: (batch_size, sequence_length, 5)\n",
    "        \n",
    "        parameter_logits = self.param_layer(x) # Shape: (batch_size, sequence_length, 6)\n",
    "        parameters_probs = F.softmax(parameter_logits, dim=-1) # Shape: (batch_size, sequence_length, 6)\n",
    "        \n",
    "        return command_probs, parameters_probs\n",
    "\n",
    "def select_parameters(command_probs, parameters_probs):\n",
    "    # command_probs: shape (batch_size, sequence_length, 5)\n",
    "    # parameters_probs: shape (batch_size, sequence_length, 6)\n",
    "    # Get the predicted command indices (shape: batch_size, sequence_length)\n",
    "    command_indx = command_probs.argmax(dim=-1).item() + 1\n",
    "    if command_indx == Commands.STOP.value:\n",
    "        parameters = torch.zeros(6).cuda()\n",
    "    elif command_indx in [Commands.MAKE_WALL.value, Commands.MAKE_DOOR.value, Commands.MAKE_WINDOW.value]:\n",
    "        parameters = parameters_probs.squeeze()\n",
    "    \n",
    "    return Commands.get_name_for(command_indx), parameters\n",
    "\n",
    "PARAM_SIZE = 6\n",
    "vocab = [\"START\", \"STOP\", \"make_wall\", \"make_window\", \"make_door\"]\n",
    "COMMANDS = vocab\n",
    "\n",
    "VOCAB_SIZE = len(vocab) + PARAM_SIZE\n",
    "# print(f\"Size of the vocabulary: {VOCAB_SIZE}\")\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, d_model).cuda()\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1).cuda()\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model)).cuda()\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "\n",
    "class CustomTransformerDecoder(nn.Module):\n",
    "    def __init__(self, d_model, nhead, num_decoder_layers, dim_feedforward, dropout=0.1):\n",
    "        super(CustomTransformerDecoder, self).__init__()\n",
    "        self.decoder_layer = nn.TransformerDecoderLayer(d_model, nhead, dim_feedforward, dropout).cuda()\n",
    "        self.transformer_decoder = nn.TransformerDecoder(self.decoder_layer, num_decoder_layers).cuda()\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, tgt, memory, tgt_mask=None, memory_mask=None, tgt_key_padding_mask=None, memory_key_padding_mask=None):\n",
    "        output = self.transformer_decoder(tgt, memory, tgt_mask=tgt_mask, memory_mask=memory_mask,\n",
    "                                          tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=memory_key_padding_mask)\n",
    "        return output\n",
    "\n",
    "def construct_embedding_vector_from_vocab(command: Commands, parameters: torch.Tensor, d_model=512):\n",
    "    num_classes = len(Commands)\n",
    "    one_hot_tensor = F.one_hot(torch.tensor(command.value - 1).cuda(), num_classes=num_classes)\n",
    "\n",
    "    if parameters.size(0) < 6:\n",
    "        parameters = torch.cat((parameters, torch.zeros(6 - parameters.size(0)).cuda()))\n",
    "\n",
    "    combined_tensor = torch.cat((one_hot_tensor, parameters)).float()\n",
    "\n",
    "    # Project to the correct dimension\n",
    "    if combined_tensor.size(0) < d_model:\n",
    "        combined_tensor = F.pad(combined_tensor, (0, d_model - combined_tensor.size(0)))\n",
    "\n",
    "    return combined_tensor\n",
    "\n",
    "class CommandTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size= VOCAB_SIZE, d_model=512, nhead=8, num_layers=6):\n",
    "        super(CommandTransformer, self).__init__()\n",
    "        self.point_cloud_encoder = PointCloudTransformerLayer().cuda()\n",
    "        self.pos_encoder = PositionalEncoding(d_model).cuda()\n",
    "        self.transformer = CustomTransformerDecoder(d_model, nhead, num_layers, 2048).cuda()\n",
    "        self.output_layer = TransformerOutputLayer(d_model).cuda()\n",
    "\n",
    "    def forward(self, src: torch.Tensor, tgt: torch.Tensor):\n",
    "        src_emb = self.point_cloud_encoder(src).cuda()\n",
    "        print(src_emb.shape)\n",
    "        # tgt_emb = self.embedding_layer(tgt).cuda()\n",
    "        tgt_emb = self.pos_encoder(tgt).cuda()\n",
    "        transformer_output = self.transformer(src_emb, tgt_emb)  # (tgt_seq_len, batch_size, d_model)\n",
    "        outputs = self.output_layer(transformer_output)  # (tgt_seq_len, batch_size, vocab_size)\n",
    "        return outputs\n",
    "\n",
    "model = CommandTransformer().cuda()\n",
    "input_emb = construct_embedding_vector_from_vocab(Commands.START, torch.zeros(6).cuda()).unsqueeze(-1).cuda()\n",
    "\n",
    "while True:\n",
    "    pred = model(point_cloud_df, input_emb)\n",
    "    command, parameters = select_parameters(*pred)\n",
    "    output_emb = construct_embedding_vector_from_vocab(command, parameters).cuda()\n",
    "    input_emb = torch.cat(input_emb, output_emb.unsqueeze(-1)).cuda()\n",
    "\n",
    "    if command == Commands.STOP:\n",
    "        break\n",
    "\n",
    "print(input_emb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
